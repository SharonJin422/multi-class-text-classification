#### 1. 数据处理阶段
* 1） matplotlib.pyplot可视化数据的分布，对数据的基本情况有所了解
* 2） 样本不均衡：样本量大的类别会在主导训练过程
  * 如果样本不均衡，可以进行数据增强，对于label比较少的样本，进行扩充，扩充方法有随机删除，随机替换同义词，随机交换
  * 可以进行类别均衡采样
  * OHEM（online hard example miniing)：根据输入样本的损失进行筛选，筛选出hard example，然后将筛选得到的样本应用在随机梯度下降中，计算损失并回传梯度，加强loss大的样本的训练
  * focal loss：OHEM是选取了loss比较大的hard example，但是也有一些loss比较小的easy 负样本，积少成多，对总Loss有一定的贡献，所以focal loss通过公式是选取了所有的负样本， 但是根据难度给了不同的权重，通过这种方式平衡正负样本
    * 为什么focal loss可以改变网络更关注hard example？ 
      * 首先理解损失函数是如何指导网络学习特征的：不同的神经元会学习到样本的一个特征，加上神经元的权重，线性累加(w1*x1+w2*x2+w3*x3....)就得到了预测值y，根据和目标函数（损失函数）的差，通过梯度来指导神经元的权重w；
      * 其次，为什么focal loss会更关注hard example：因为当Pt越接近1的时候，focal loss越小，所以对正样本的关注就更少
   * 对数据做shuffle
 * 3）如何处理小样本+多分类的情况，或者是某几个类别样本数量特别少
    * few-shot classification, low-shot classification![image](https://user-images.githubusercontent.com/85158796/137271493-8b219626-8920-4689-9d19-4a0fd145db05.png)

#### 2. 训练小技巧
##### 2.1 学习率如何调整
* tf.train.exponential_decay 可以实现指数衰减法
* pytorch 还提供了ReduceLROnPlateau可以监督学习率，根据loss升高或者降低的次数来动态更新学习率
* 等间距调整学习率StepLR
* 批次大小和学习率的关系：batch size越大，初始的learning rate也越大
* 如何找到一个合适的学习率How Do You Find A Good Learning Rate https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html
![image](https://user-images.githubusercontent.com/85158796/137271919-4a852ff5-70bc-4c8a-8927-2db8fa733325.png)

##### 2.2 embedding层的处理
* 1）优化器
    * Optimizer.minimize(添加操作节点，用于最小化loss)等价于tf.train.Optimizer.compute_gradients + tf.train.Optimizer.apply_gradients，同时如果global_step非None, 会进行global_step的自增操作
    * 引入tf.clip_by_global_norm进行梯度裁剪，使权重的更新限制在一个合适的范围，避免梯度消失和爆炸
* 2）embedding层的格式：要先生成字典，然后根据字典的顺序，生成每个词对应的embedding matrix, 然后每个句子对应的[idx1,idx2,idx3]去查找对应的词向量，主要是为了Index索引对应的词一样， embedding_lookup可以理解成一个全连接层
![image](https://user-images.githubusercontent.com/85158796/137272063-7a9b2a16-909e-47e1-8c02-9c57d4d48fe6.png)

##### 2.3 其他
* label是one-hot形式和单个值，对应的loss函数也不一样：tf.nn.sparse_softmax_cross_entropy_with_logits 和tf.nn.softmax_cross_entropy_with_logits
* ensemble：不同的模型，进行线性融合，比如RNN和传统模型
* 在全连接层采用dropout（防止过拟合过多的参数，参数越多，drop prob越大，使隐藏的单元激活为零），在卷积层不使用，因为卷积层的参数并不多
* 把分类错误的样本（hard negative)在模型训练完后再进行fine-tune


#### 3. 网络结构
##### 3.1 TextCNN
* input layer：对于文本任务,输入层采用了word embedding
* convolutional layer
    * 1)卷积核只在高度上滑动（即不同的单词），宽度上和一个单词的word embedding dim保持一致（=200），保证了word作为最小的粒度
    * 2）由于卷积核和word embedding的宽度一致，所以卷积后输出的vector.shape= \[sentence_len - filter_window + 1, 1\]， max-pooling之后得到的是一个scalar标量
    * 3）为了获取不同维度的特征，convolution filter的数量可以自行设置，convolution kernel size 一般包括3*3, 4*4, 5*5
* max-pooling layer: 因为每个卷积输出经过maxpooling后只得到一个scalar，所以实施了n个filter_window_size(不同的窗口大小)，每个window_size又有num_filter个卷积核,，最终得到num_filters\*n个scalar，再concat一起得到vector，送入softmax layer
* softmax layer:为了避免过拟合，在softmax的时候进行dropout（只在训练阶段，测试阶段不用dropout）
      
##### 3.2 FastText 机器学习模型，但是不需要构建特征工程,总的来说是运用CBOW构建词向量后，应用层次softmax预测标签
* 1）网络结构和CBOW一样，CBOW是根据上下文预测中间词，而fatsText是构建句子特征预测标签
* 2）层次softmax：对于类别很大的时候，softmax需要对所有的类别归一，耗时很大， 可以进行层次softmax，构建霍夫曼树替代标准的softmax, 复杂度从N变为logN；或者是negative sampling softmax近似计算softmax
* 3) N-gram特征
![image](https://user-images.githubusercontent.com/85158796/137272620-5d8258fc-1e49-42e0-83b8-72e560bbcfe6.png)

#### 4. 评估指标
* macro f1:先计算每个类别的f1 score，再计算均值
* micro f1:不区分类别，直接求整体的f1 score
* ROC曲线
* Kappa
* 区分准确率（accuracy)和精准率（precision）
![image](https://user-images.githubusercontent.com/85158796/137273017-42024864-3323-41a6-b49c-e0339e64055a.png)

#### 5. 损失函数
##### 5.1 多分类、多标签（multi-label）问题的损失函数和二分类的损失函数一样，可以把multi-label的输出的每个类别都当做二分类，输出是一个类别数量长度c的01序列
* 损失函数 binary_crossentropy
![image](https://user-images.githubusercontent.com/85158796/137273303-7f42e12b-2195-4b5c-bccb-e094a54e6950.png)

##### 5.2 多分类、单标签问题(multi-class)
* 损失函数categorcal_crossentropy:
![image](https://user-images.githubusercontent.com/85158796/137273344-e6e6e214-752c-45b0-95e5-528c7280f1d1.png)

#### 6. 如何判断达到最优解
* 多跑几次，初始值不一样，模型跑出来的结果也不一样
* 尝试SGD，加入扰动，更容易从鞍点出来
* 观察模型在测试集上的loss曲线分布
  * overffit时候training loss一直在不断地下降，而validation loss在某个点开始不再下降反而开始上升了，这就说明overfit
  * Good git在loss曲线上的特点是training loss和validation loss都已经收敛并且之间相差很小很小。
![image](https://user-images.githubusercontent.com/85158796/137273473-471b92b5-d045-4769-92ff-a1180d46b27a.png)

#### 7.模型收敛后，还可以尝试什么
* 优化网络结构，使用更好的backbone
* 使用更好的LOSS函数
* 考虑使用难例挖掘的方法，比如OHEM
* 观察测试样本，查找case，针对Case来，补充样本+数据增强
* 不同的权重初始化方法
* 尝试不同的学习率初始值和衰减值
* 考虑在梯度上做文章，可以是梯度裁剪、梯度校验、梯度归一化等方法![image](https://user-images.githubusercontent.com/85158796/137273565-a61d2322-fd6b-4914-93d3-100e341f31f1.png)

#### 总结
* 短文本：RNN比CNN效果好
* 长文本：CNN、tf-idf + SVM等传统的机器学习方法、HAN
![image](https://user-images.githubusercontent.com/85158796/137274332-e2c05e8e-4b77-445e-97e0-6154e0ee74f3.png)
